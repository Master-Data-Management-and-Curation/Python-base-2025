{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 6: Training Models with PyTorch - Loss and Optimizers\n",
    "\n",
    "**Objective:** To understand the fundamental training loop in machine learning by using PyTorch's `autograd` to minimize a loss function, and then to learn how `torch.optim` simplifies this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The Goal: Minimizing Loss\n",
    "\n",
    "Machine learning is essentially about adjusting a model's parameters (like a weight `w`) so that its predictions get closer and closer to the true values. We measure how 'wrong' the model is using a **loss function**. Our goal is to find the parameters that make the loss as small as possible.\n",
    "\n",
    "The process looks like this:\n",
    "1.  **Forward Pass:** Make a prediction.\n",
    "2.  **Compute Loss:** Compare the prediction to the true value.\n",
    "3.  **Backward Pass:** Calculate the gradient of the loss with respect to the model's parameters. This tells us the direction to adjust our parameters to reduce the loss.\n",
    "4.  **Update Parameters:** Adjust the parameters in the opposite direction of the gradient.\n",
    "\n",
    "We repeat this process iteratively. This is called **Gradient Descent**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Manual Gradient Descent (Single Data Point)\n",
    "\n",
    "Let's solve a simple problem: `y = w * x`. We have a single data point `x=2` and a target `y=4`. We know the answer should be `w=2`, but let's pretend we don't and make the model learn it.\n",
    "\n",
    "Our loss function will be the **Mean Squared Error (MSE)**: `loss = (prediction - y)^2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "((Important: Why is MSE a \"good\" loss function?))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the abstract loop to follow to *learn* w *given* x,y ?\n",
    "\n",
    "- Calculate the prediction given the parameter w.\n",
    "- Calculate how wrong the prediction is (the loss).\n",
    "- Calculate what is the direction to reduce the wrongness (the grad of the loss)\n",
    "- Update the w in the rection of the gradient\n",
    "- Re-calculate the prediction and loop again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# --- Setup ---\n",
    "# Known data point\n",
    "x = torch.tensor(2.0)\n",
    "y = torch.tensor(4.0)\n",
    "\n",
    "# Initialize a random weight. We must set requires_grad=True to compute gradients.\n",
    "w = torch.tensor(0.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = w * x\n",
    "loss = (y_pred - y)**2\n",
    "\n",
    "print(f\"x is {x} with grad {x.grad}\")\n",
    "print(f\"y is {y} with grad {y.grad}\")\n",
    "print(f\"y_pred is {y_pred} with grad {y_pred.grad}\")\n",
    "print(f\"loss is {loss} with grad {loss.grad}\")\n",
    "print(f\"w is {w} with grad {w.grad}\")\n",
    "\n",
    "loss.backward()\n",
    "# d loss / d w = 2 * (y_pred - y) * x\n",
    "print(\"\\nmanual gradient is:\")\n",
    "print(\"d loss / d w = 2 * (y_pred - y) * x: \", 2 * (y_pred - y) * x)\n",
    "\n",
    "print(f\"\\n autograd. doing backward pass!\\n\\n\")\n",
    "\n",
    "print(f\"x is {x} with grad {x.grad}\")\n",
    "print(f\"y is {y} with grad {y.grad}\")\n",
    "print(f\"y_pred is {y_pred} with grad {y_pred.grad}\")\n",
    "print(f\"loss is {loss} with grad {loss.grad}\")\n",
    "print(f\"w is {w} with grad {w.grad}\")\n",
    "\n",
    "print(\"\\nmanual update is:\")\n",
    "print(\"w -> w - alpha * d loss/ d w\")\n",
    "# w -> w - 0.1 * w.grad\n",
    "\n",
    "print(\"new prediction is: \", (w-0.1*w.grad)*x)\n",
    "\n",
    "print(f\"\\nresetting grad to zero.\\n\\n\")\n",
    "w.grad.zero_()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter: how big of a step we take during each update\n",
    "learning_rate = 0.01\n",
    "print(f\"Initial prediction: f(2) = {w * x:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Loop ---\n",
    "for epoch in range(40):\n",
    "    # 1. Forward pass: make a prediction\n",
    "    y_pred = w * x\n",
    "    \n",
    "    # 2. Compute loss\n",
    "    loss = (y_pred - y)**2\n",
    "    \n",
    "    # 3. Backward pass: compute gradient of loss w.r.t. w\n",
    "    loss.backward() # Populates w.grad\n",
    "    \n",
    "    # 4. Manually update weight using the gradient\n",
    "    # We wrap this in `with torch.no_grad()` because we don't want this update to be tracked.\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "    \n",
    "    # **VERY IMPORTANT**: Zero the gradients after updating\n",
    "    # If we don't, gradients will accumulate from all previous steps.\n",
    "    w.grad.zero_()\n",
    "    \n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        print(f'Epoch {epoch+1}: w = {w:.3f}, loss = {loss:.8f}')\n",
    "\n",
    "print(f\"\\nFinal prediction: f(2) = {w * x:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Gradient Descent with a Batch of Data\n",
    "\n",
    "Usually, we have more than one data point. The process is the same, but now our forward and backward passes operate on a whole **batch** of data at once. The loss is typically the average loss over the entire batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup for Batch ---\n",
    "# True relationship: y = 2*x\n",
    "X = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "Y = torch.tensor([2.0, 4.0, 6.0, 8.0])\n",
    "\n",
    "# Initialize a random weight\n",
    "w = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "# --- Training Loop for Batch ---\n",
    "for epoch in range(40):\n",
    "    # 1. Forward pass (works on the whole batch)\n",
    "    Y_pred = w * X\n",
    "    \n",
    "    # 2. Compute loss (average over the batch)\n",
    "    loss = torch.mean((Y_pred - Y)**2)\n",
    "    \n",
    "    # 3. Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # 4. Manual update\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        \n",
    "    # Zero the gradients\n",
    "    w.grad.zero_()\n",
    "    \n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        print(f'Epoch {epoch+1}: w = {w:.3f}, loss = {loss:.8f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Using a PyTorch Optimizer\n",
    "\n",
    "Manually updating weights and zeroing gradients is repetitive. PyTorch provides **optimizers** in `torch.optim` that handle this for us.\n",
    "\n",
    "The two key methods are:\n",
    "- `optimizer.step()`: Updates all the parameters that were passed to the optimizer during its creation.\n",
    "- `optimizer.zero_grad()`: Sets the gradients of all managed parameters to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup with Optimizer ---\n",
    "X = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "Y = torch.tensor([2.0, 4.0, 6.0, 8.0])\n",
    "w = torch.tensor(0.0, requires_grad=True)\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Define the model (for this simple case, it's just a multiplication)\n",
    "def model(x):\n",
    "    return w * x\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Define the optimizer\n",
    "# We pass it the parameters it should manage ([w]) and the learning rate.\n",
    "optimizer = torch.optim.SGD([w], lr=learning_rate)\n",
    "\n",
    "# --- Training Loop with Optimizer ---\n",
    "for epoch in range(20):\n",
    "    # 1. Forward pass\n",
    "    Y_pred = model(X)\n",
    "    \n",
    "    # 2. Compute loss\n",
    "    loss = loss_fn(Y_pred, Y)\n",
    "    \n",
    "    # 3. Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # 4. Update weights with optimizer\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Zero the gradients with optimizer\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        print(f'Epoch {epoch+1}: w = {w:.3f}, loss = {loss:.8f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Exercises & Debugging (90 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 6.1: Learning a Linear Model (`y = w*x + b`)\n",
    "\n",
    "* **Task:** The true relationship between our data is now `y = 3*x + 2`. Your goal is to find the correct values for both `w` and `b`.\n",
    "  1.  Set up the data `X` and `Y` based on the true relationship.\n",
    "  2.  Initialize two parameters, `w` and `b`, to random values (or zero) and make sure they `require_grad`.\n",
    "  3.  Set up an `SGD` optimizer to manage **both** `w` and `b`.\n",
    "  4.  Complete the training loop to learn the values of `w` and `b`.\n",
    "  5.  Print the final learned `w` and `b` after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Your Code Here ---\n",
    "\n",
    "# 1. Setup Data\n",
    "X_true = torch.arange(10, dtype=torch.float32)\n",
    "Y_true = 3 * X_true + 2 # True relationship\n",
    "\n",
    "# 2. Initialize parameters\n",
    "w = torch.tensor(0.0, requires_grad=True)\n",
    "b = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "n_epochs = 50\n",
    "\n",
    "# 3. Define Optimizer (managing both w and b)\n",
    "optimizer = torch.optim.SGD([w, b], lr=learning_rate)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# 4. Training Loop\n",
    "for epoch in range(n_epochs):\n",
    "    # Forward pass\n",
    "    Y_pred = w * X_true + b\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = loss_fn(Y_pred, Y_true)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update parameters and zero gradients\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'Epoch {epoch+1}: w = {w:.3f}, b = {b:.3f}, loss = {loss:.4f}')\n",
    "\n",
    "# 5. Print final results\n",
    "print(\"\\n--- Final Learned Parameters ---\")\n",
    "print(f\"Learned w: {w:.4f} (True value is 3)\")\n",
    "print(f\"Learned b: {b:.4f} (True value is 2)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
