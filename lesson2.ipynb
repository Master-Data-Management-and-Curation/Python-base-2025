{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2: Pandas : Ingestion, Cleaning, and Aggregation\n",
    "\n",
    "**Objective:** To gain insights in using pandas to load, clean, transform, and analyze data from various sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pandas sqlalchemy pandas-datareader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Broad Concepts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does pandas stands for?\n",
    "Apparently:\n",
    "\n",
    "*'The name is derived from the term \"panel data\", an econometrics term for data sets that include observations over multiple time periods for the same individuals,[3] as well as a play on the phrase \"Python data analysis\"'* (wikipedia)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Ensure the necessary data files exist\n",
    "!python3 database_setup.py # This runs the script to create the database and csv files\n",
    "\n",
    "# From CSV\n",
    "df_messy = pd.read_csv('messy_sales_data.csv')\n",
    "print(\"--- Messy Sales Data Head ---\")\n",
    "print(df_messy.head())\n",
    "\n",
    "# From SQL\n",
    "engine = create_engine('sqlite:///company.db')\n",
    "df_emp = pd.read_sql(\"SELECT * FROM employees\", engine)\n",
    "print(\"\\n--- Employees Data Head ---\")\n",
    "print(df_emp.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas allows to efficiently work on data, \"Ã  la Excel\". \n",
    "The entire object is called a ```pandas DataFrame``` and the single columns are called ```Series```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nData entries:\\n\", df_messy.keys())\n",
    "print(\"\\nData types:\\n\", df_messy.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select the column, use the column label (the one you see called by the ```keys()``` function above) in between square brackets []."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( df_messy['Price'], \"\\n\\nand the type is\\n\\n\", df_messy['Price'].dtype )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Cleaning & Preparation\n",
    "This is the most time-consuming part of data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handling Missing Data (`NaN`)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values before cleaning:\")\n",
    "print(df_messy.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill numeric NaNs with the column median by reassigning the result\n",
    "median_price = df_messy['Price'].median()\n",
    "df_messy['Price'] = df_messy['Price'].fillna(median_price)\n",
    "\n",
    "median_qty = df_messy['Quantity'].median()\n",
    "df_messy['Quantity'] = df_messy['Quantity'].fillna(median_qty)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill categorical NaNs with a placeholder by reassigning the result\n",
    "df_messy['Region'] = df_messy['Region'].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMissing values after cleaning:\")\n",
    "print(df_messy.isnull().sum())\n",
    "\n",
    "print(df_messy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Strong-arm handling the Missing Data**:\n",
    "\n",
    "The method ```dropna``` instead allows to delete completely the data where ```NaN``` are present. This can be done at very degrees of aggressiveness, for example:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_messy_again = pd.read_csv('messy_sales_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove all entries with even a single NaN\n",
    "print( df_messy_again.dropna() )\n",
    "\n",
    "#Remove all columns with even a single NaN\n",
    "print(\"\\n\", df_messy_again.dropna(axis='columns') )\n",
    "\n",
    "#Remove all entries with NaNs but only in the column 'Price'\n",
    "print(\"\\n\", df_messy_again.dropna(subset='Price') )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correcting Data Types**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data types before:\", df_messy.dtypes)\n",
    "df_messy['Quantity'] = df_messy['Quantity'].astype(int)\n",
    "df_messy['Date'] = pd.to_datetime(df_messy['Date'])\n",
    "print(\"\\nData types after:\", df_messy.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**String Manipulation (`.str`)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Region values before cleanup:\", df_messy['Region'].unique())\n",
    "# Clean whitespace and standardize case\n",
    "df_messy['Region'] = df_messy['Region'].str.strip().str.title()\n",
    "print(\"\\nRegion values after cleanup:\", df_messy['Region'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handling Duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape before dropping duplicates: {df_messy.shape}\")\n",
    "df_messy.drop_duplicates(inplace=True)\n",
    "print(f\"Shape after dropping duplicates: {df_messy.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Transformation & Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Applying Custom Functions (`.apply`)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_price(price):\n",
    "    return 'High' if price > 200 else 'Low'\n",
    "\n",
    "df_messy['PriceCategory'] = df_messy['Price'].apply(categorize_price)\n",
    "print(df_messy[['Price', 'PriceCategory']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grouping Data (`.groupby`)**: Essential for summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average price per region\n",
    "avg_price_by_region = df_messy.groupby('Region')['Price'].mean()\n",
    "print(avg_price_by_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combining Datasets (`.merge`)**: SQL-style joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dept = pd.read_csv('departments.csv')\n",
    "# Merge employee data with department data\n",
    "merged_df = pd.merge(df_emp, df_dept, on='department_id', how='left')\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Basic Statistical Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statistical info of the dataframe (`.describe`)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['salary'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performing other operations on data (`.agg`)**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions that return a single value\n",
    "print( merged_df['salary'].agg(['mean', 'median']) )\n",
    "\n",
    "# Function that return a transformation of the dataset\n",
    "print( \"\\n\", merged_df['salary'].agg(['cumsum', 'cumprod']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You cannot mix and match these two types...\n",
    "\n",
    "\n",
    "print( \"\\n\", merged_df['salary'].agg(['cumsum', 'sum']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Exercises "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2.1: Comprehensive Data Cleanup and Analysis\n",
    "* **Task:** Perform a full analysis pipeline on the company data.\n",
    "  1. Load `employees.csv` and `departments.csv`.\n",
    "  2. Merge the two DataFrames into one, using the department_id as the common key.\n",
    "  3. Create a new column `salary_level` which is 'High' for salaries > 90000 and 'Standard' otherwise.\n",
    "  4. Use `groupby()` to calculate the number of employees and the average salary for each `department_name`.\n",
    "  5. Print the final report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load data\n",
    "df_emp = pd.read_csv('employees.csv')\n",
    "df_dept = pd.read_csv('departments.csv')\n",
    "\n",
    "# 2. Merge data\n",
    "\n",
    "# 3. Create 'salary_level' column\n",
    "\n",
    "# 4. Group and aggregate\n",
    "\n",
    "# 5. Print report\n",
    "print(\"Company Department Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2.2: Playing around with real data\n",
    "* **Task:** Perform exploratory analysis on real world economic data.\n",
    "You will be given the code to use `pandas_reader` to download and clean a public available dataset of economic data.\n",
    "  1. Explore how a real database is managed ( https://databank.worldbank.org/databases ) .\n",
    "  2. Explore the numerics of the dataset, using the pandas internal correlation calculation to compute the correlation between gdp and life expectancy, and the correlation between the change of these indicators in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_datareader import wb\n",
    "\n",
    "# Define indicators and the time range\n",
    "indicators = {'NY.GDP.PCAP.KD': 'gdp_per_capita', 'SP.DYN.LE00.IN': 'life_expectancy'}\n",
    "start_year = 2019\n",
    "end_year = 2023\n",
    "\n",
    "# Fetch data from World Bank API\n",
    "data = wb.download(indicator=list(indicators.keys()), \n",
    "                   country='all', \n",
    "                   start=start_year, \n",
    "                   end=end_year)\n",
    "\n",
    "# Rename columns for clarity\n",
    "data = data.rename(columns=indicators)\n",
    "\n",
    "print(\"Original Data Sample:\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index to turn 'country' and 'year' into columns\n",
    "df = data.reset_index()\n",
    "\n",
    "# Drop rows with any missing values for our indicators\n",
    "df_clean = df.dropna(subset=['gdp_per_capita', 'life_expectancy'])\n",
    "\n",
    "# Display info and a sample of the cleaned data\n",
    "print(\"\\nCleaned Data Info:\")\n",
    "df_clean.info()\n",
    "\n",
    "print(\"\\nCleaned Data Sample:\")\n",
    "print(df_clean.head())\n",
    "\n",
    "# Get country metadata to filter out aggregates\n",
    "countries_info = wb.get_countries()\n",
    "# Filter for countries only (regions have 'agg' in the 'region' column)\n",
    "country_codes = countries_info[countries_info['region'] != 'Aggregates']['iso3c'].tolist()\n",
    "\n",
    "# Filter our main dataframe to include only countries\n",
    "df_countries = df_clean[df_clean['country'].isin(countries_info[countries_info['region'] != 'Aggregates']['name'])]\n",
    "\n",
    "\n",
    "print(f\"\\nOriginal rows: {len(df_clean)}, Rows after filtering for countries: {len(df_countries)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
