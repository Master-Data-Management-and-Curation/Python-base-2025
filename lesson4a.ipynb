{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 4: SciPy & Introduction to Scikit-learn\n",
    "\n",
    "**Objective:** To explore the SciPy library for scientific computing and introduce Scikit-learn for foundational machine learning tasks like clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Concepts (90 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is SciPy?\n",
    "SciPy is a library that builds on NumPy and provides a large number of higher-level algorithms for scientific and technical computing. While NumPy's focus is on the `ndarray` object, SciPy provides modules for optimization, linear algebra, integration, signal processing, and, importantly for us, statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. `scipy.stats`: Statistical Functions\n",
    "This submodule contains a wide range of statistical functions and probability distributions. A common use case is performing hypothesis tests, such as a T-test, which is used to determine if there is a significant difference between the means of two groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "# Sample data for two groups (e.g., test scores)\n",
    "group_a_scores = np.random.normal(loc=85, scale=5, size=30)\n",
    "group_b_scores = np.random.normal(loc=88, scale=5, size=30)\n",
    "\n",
    "# Perform an independent T-test\n",
    "t_statistic, p_value = stats.ttest_ind(group_a_scores, group_b_scores)\n",
    "\n",
    "print(f\"T-statistic: {t_statistic:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "if p_value < 0.05: # A common significance level\n",
    "    print(\"The difference between the groups is statistically significant.\")\n",
    "else:\n",
    "    print(\"The difference between the groups is not statistically significant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. `scipy.optimize`: Curve Fitting\n",
    "The `optimize` module provides functions for finding the minimum or root of a function. A very common application is curve fitting, where we find the parameters of a function that best fit a set of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the function we want to fit (a quadratic function)\n",
    "def func(x, a, b, c):\n",
    "    return a * x**2 + b * x + c\n",
    "\n",
    "# Generate some noisy data\n",
    "x_data = np.linspace(0, 4, 50)\n",
    "y_data = func(x_data, 2.5, 1.3, 0.5) + np.random.normal(0, 2.0, size=len(x_data))\n",
    "\n",
    "# Fit the curve\n",
    "params, covariance = curve_fit(func, x_data, y_data)\n",
    "print(f\"Fitted parameters (a, b, c): {params}\")\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(x_data, y_data, label='Data')\n",
    "plt.plot(x_data, func(x_data, *params), color='red', label='Fitted function')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Introduction to Scikit-learn (SKLearn)\n",
    "Scikit-learn is the most popular library for machine learning in Python. It provides simple and efficient tools for data mining and data analysis. It's built on NumPy, SciPy, and Matplotlib.\n",
    "\n",
    "**Key features of SKLearn:**\n",
    "- **Consistent API:** Models are Python classes with `fit()`, `predict()`, and `transform()` methods.\n",
    "- **Wide Range of Algorithms:** It covers classification, regression, clustering, dimensionality reduction, and more.\n",
    "- **Excellent Documentation:** Its documentation is considered a gold standard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Clustering with `sklearn.cluster`\n",
    "While SciPy has clustering algorithms, SKLearn's implementations are more feature-rich and follow the consistent API. K-Means is a popular algorithm that groups data by trying to separate samples into *k* groups of equal variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "# Create some synthetic data with two distinct groups\n",
    "group1 = np.random.randn(50, 2) + np.array([5, 5])\n",
    "group2 = np.random.randn(50, 2) + np.array([0, 0])\n",
    "data = np.vstack([group1, group2])\n",
    "\n",
    "# Step 1: Scale the data (SKLearn's version of 'whitening')\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Step 2: Create and fit the K-Means model\n",
    "kmeans = KMeans(n_clusters=2, random_state=42, n_init=10) # n_init suppresses a future warning\n",
    "kmeans.fit(scaled_data)\n",
    "\n",
    "# Step 3: Get the cluster assignments (labels)\n",
    "cluster_ids = kmeans.labels_\n",
    "\n",
    "# Visualize the results\n",
    "sns.scatterplot(x=data[:, 0], y=data[:, 1], hue=cluster_ids, palette='viridis')\n",
    "plt.title('K-Means Clustering Result with SKLearn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Dataset Management with Scikit-learn\n",
    "Often you need to practice with well-known datasets. The `scikit-learn` library provides easy access to many of them. These datasets are loaded as a special `Bunch` object, which contains the data, target, and descriptions all in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Explore the Bunch object\n",
    "print(iris.keys())\n",
    "print(iris.DESCR[:500] + \"...\") # Print the first 500 characters of the description\n",
    "\n",
    "# The best practice is to convert it to a pandas DataFrame for analysis\n",
    "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "iris_df['target'] = iris.target\n",
    "\n",
    "print(\"\\nIris dataset converted to a DataFrame:\")\n",
    "print(iris_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Exercises & Debugging (90 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 4.1: T-Test on Employee Salaries\n",
    "* **Task:** Load the company dataset. Perform a T-test to see if there is a significant difference in salary between the 'Engineering' and 'Sales' departments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Load data\n",
    "df_emp = pd.read_csv('employees.csv')\n",
    "df_dept = pd.read_csv('departments.csv')\n",
    "company_df = pd.merge(df_emp, df_dept, on='department_id')\n",
    "\n",
    "# Isolate the salaries for the two departments\n",
    "eng_salaries = company_df[company_df['department_name'] == 'Engineering']['salary']\n",
    "sales_salaries = company_df[company_df['department_name'] == 'Sales']['salary']\n",
    "\n",
    "# Perform T-test\n",
    "t_stat, p_val = stats.ttest_ind(eng_salaries, sales_salaries)\n",
    "\n",
    "print(f\"T-test results for Engineering vs. Sales salaries:\")\n",
    "print(f\"P-value: {p_val:.4f}\")\n",
    "if p_val < 0.05:\n",
    "    print(\"Conclusion: There is a significant difference in salaries.\")\n",
    "else:\n",
    "    print(\"Conclusion: There is no significant difference in salaries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 4.2: Clustering Employees with SKLearn\n",
    "* **Task:** Use `sklearn.cluster.KMeans` to segment employees into groups based on their `salary` and `tenure_years`. Try to find 3 distinct clusters and visualize the result. Remember to scale the data first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Prepare the data: select the two features\n",
    "employee_features = company_df[['salary', 'tenure_years']].values\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(employee_features)\n",
    "\n",
    "# Create and fit the K-Means model for 3 clusters\n",
    "kmeans_model = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "cluster_ids = kmeans_model.fit_predict(scaled_features)\n",
    "\n",
    "# Add the cluster ID back to the DataFrame for plotting\n",
    "company_df['cluster'] = cluster_ids\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(data=company_df, x='tenure_years', y='salary', hue='cluster', palette='deep')\n",
    "plt.title('Employee Segments by Salary and Tenure (k=3)')\n",
    "plt.xlabel('Tenure (Years)')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 4.3: Load and Explore the Wine Dataset\n",
    "* **Task:** Use `sklearn.datasets.load_wine()` to load the wine dataset. Convert it into a pandas DataFrame and print the first 5 rows (`.head()`) and the dimensions of the DataFrame (`.shape`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "wine = load_wine()\n",
    "\n",
    "# Convert to DataFrame\n",
    "wine_df = pd.DataFrame(data=wine.data, columns=wine.feature_names)\n",
    "wine_df['target'] = wine.target\n",
    "\n",
    "# Print head and shape\n",
    "print(\"Wine Dataset Head:\")\n",
    "print(wine_df.head())\n",
    "print(\"\\nWine Dataset Shape:\", wine_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging Focus: Choosing 'k' in K-Means\n",
    "- **The Problem:** The biggest challenge in K-Means is choosing the right number of clusters, `k`. A bad `k` can lead to meaningless clusters. In our lab, we chose `k=3` arbitrarily, but how could we do better?\n",
    "- **The \"Elbow Method\":** A common heuristic is to run K-Means for a range of `k` values (e.g., 1 to 10) and plot the *inertia* for each `k`. Inertia is the sum of squared distances of samples to their closest cluster center. SKLearn's KMeans model stores this value in the `inertia_` attribute after fitting. The plot often looks like an arm, and the \"elbow\" (the point of inflection where the rate of decrease sharply shifts) is a good candidate for `k`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
