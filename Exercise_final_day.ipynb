{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d345048-4cfc-457b-8e74-734a35a3efca",
   "metadata": {},
   "source": [
    "## PyTorch Transfer Learning Exercise with Hugging Face & CIFAR-10\n",
    "\n",
    "### Objective:\n",
    "Learn the fundamentals of transfer learning by loading a pre-trained visual model,\n",
    "adding a custom classification layer, and training it on the CIFAR-10 dataset.\n",
    "\n",
    "Follow the steps below, filling in the required code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8b4f10-2153-4620-94fb-2275598fd5c0",
   "metadata": {},
   "source": [
    "Your aim is to populate the functions that compose all steps of this training run:\n",
    "\n",
    "```# This block ties everything together.\n",
    "print(\"--- PyTorch Transfer Learning Exercise ---\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 1. Get Dataloaders\n",
    "train_loader, test_loader = get_cifar10_dataloaders()\n",
    "\n",
    "# 2. Load Base Model\n",
    "base_model = load_pretrained_model()\n",
    "\n",
    "# 3. Create Custom Model\n",
    "cifar_model = EfficientNetCIFAR10(base_model)\n",
    "\n",
    "# 4. Define Loss and Optimizer\n",
    "# We only want to train the parameters of our new classifier layer.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cifar_model.classifier.parameters(), lr=0.001)\n",
    "\n",
    "# 5. Train the Model\n",
    "# Note: Training for more epochs will yield better results.\n",
    "# 3 epochs is a good starting point to verify the setup works.\n",
    "train_model(cifar_model, train_loader, criterion, optimizer, device, epochs=3)\n",
    "\n",
    "# 6. Evaluate the Model\n",
    "evaluate_model(cifar_model, test_loader, device)```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe81f97-bd17-4fbd-940d-0786f77ecfef",
   "metadata": {},
   "source": [
    "### step 1: `import`\n",
    "This initial step involves importing all the necessary packages for the project.\n",
    "We import `torch` and `torch.nn` for core deep learning functionalities and building the model.\n",
    "`DataLoader` is used for efficiently loading and batching data. `torchvision` provides access\n",
    "to popular datasets like CIFAR-10 and image transformation functions. The `transformers` library\n",
    "from Hugging Face is key for easily downloading and using pre-trained models. Finally, `tqdm`\n",
    "is a utility that provides progress bars for our loops, making the training process more informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fe1f63-a6f7-4850-939b-db3589fc4c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Import Necessary Libraries ---\n",
    "# We need torch for building the neural network, torchvision for datasets and\n",
    "# transformations, and transformers from Hugging Face to load our pre-trained model.\n",
    "# tqdm is a handy utility for creating progress bars.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import AutoModel\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcb1613-afd2-42fe-b573-5e7605e0cbe0",
   "metadata": {},
   "source": [
    "### step 2. `get_cifar10_dataloaders`\n",
    "\n",
    "In this step, we prepare our data for training and testing. We first define a series of\n",
    "transformations to apply to each image. This is crucial because pre-trained models expect\n",
    "a specific input format; here, we resize CIFAR-10's small 32x32 images to the 224x224 size\n",
    "expected by EfficientNet. We also convert images to PyTorch Tensors and normalize their\n",
    "pixel values. Finally, we create `DataLoader` instances for both the training and test sets,\n",
    "which will handle batching the data, shuffling it for training, and loading it in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70df27db-c160-45b0-8ae4-ed0a3b57edbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Prepare the CIFAR-10 Dataset ---\n",
    "# We'll load the CIFAR-10 dataset and apply some transformations to prepare it\n",
    "# for the model. EfficientNet models expect a specific input size (e.g., 224x224).\n",
    "\n",
    "def get_cifar10_dataloaders():\n",
    "    \"\"\"\n",
    "    Prepares and returns the CIFAR-10 training and testing dataloaders.\n",
    "    \"\"\"\n",
    "    print(\"Step 2: Preparing CIFAR-10 Dataloaders...\")\n",
    "\n",
    "    # Define the transformations for the images.\n",
    "    # - Resize to the expected input size of the pre-trained model.\n",
    "    # - Convert the image to a PyTorch Tensor.\n",
    "    # - Normalize the tensor values to a standard range.\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # Download and load the training dataset.\n",
    "    train_dataset = ...\n",
    "\n",
    "    # Download and load the test dataset.\n",
    "    test_dataset = ...\n",
    "\n",
    "    # Create DataLoader instances to handle batching and shuffling.\n",
    "    train_loader = ...\n",
    "    test_loader = ...\n",
    "\n",
    "    print(\"Dataloaders created successfully.\")\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f0d216-9ed8-46a0-9ef7-160512af5bf6",
   "metadata": {},
   "source": [
    "### step 3. `load_pretrained_model`\n",
    "This step leverages the power of transfer learning by fetching a model that has already been\n",
    "trained on a very large dataset (like ImageNet). We use the `AutoModel.from_pretrained`\n",
    "function from the Hugging Face library to download and instantiate 'google/efficientnet-b0'.\n",
    "This model has already learned a rich set of features for recognizing various objects, which we\n",
    "can adapt for our specific task without having to train a large model from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d93d460-b411-47c2-ba5b-8c5a58e37f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Load a Pre-trained Model from Hugging Face ---\n",
    "# We will use the 'google/efficientnet-b0' model. The AutoModel class from\n",
    "# Hugging Face automatically fetches the correct model architecture.\n",
    "\n",
    "def load_pretrained_model():\n",
    "    \"\"\"\n",
    "    Loads the EfficientNet-B0 model from Hugging Face.\n",
    "    \"\"\"\n",
    "    print(\"\\nStep 3: Loading pre-trained model from Hugging Face...\")\n",
    "    model_name = \"google/efficientnet-b0\"\n",
    "    base_model = AutoModel.from_pretrained(model_name)\n",
    "    print(f\"'{model_name}' loaded successfully.\")\n",
    "    return base_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4291ef-2a58-4c33-b9cc-4e37e72d3613",
   "metadata": {},
   "source": [
    "### step 4. `EfficientNetCIFAR10`\n",
    "\n",
    " Here we define our new model architecture. The core idea is to use the pre-trained model as a\n",
    " fixed feature extractor. We achieve this by \"freezing\" all the parameters of the base model\n",
    " (`param.requires_grad = False`), so they won't be updated during training. Then, we add a new,\n",
    " trainable `nn.Linear` layer on top. This layer, our \"classifier,\" is the only part of the model\n",
    " that will learn. It takes the high-level features extracted by the base model and learns to map\n",
    " them to the 10 classes of the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944b6084-8c1e-4368-ace4-49845de329df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Build the Custom Classifier Model ---\n",
    "# Here, we'll define a new PyTorch module. This module will contain the\n",
    "# pre-trained EfficientNet as its base and a new, trainable linear layer\n",
    "# on top, which will act as our CIFAR-10 classifier.\n",
    "\n",
    "class EfficientNetCIFAR10(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        \"\"\"\n",
    "        Initializes the custom classifier model.\n",
    "        Args:\n",
    "            base_model: A pre-trained model from Hugging Face.\n",
    "        \"\"\"\n",
    "        super(EfficientNetCIFAR10, self).__init__()\n",
    "        print(\"\\nStep 4: Building the custom classifier...\")\n",
    "\n",
    "        self.base_model = base_model\n",
    "        # The number of output classes for CIFAR-10 is 10.\n",
    "        num_classes = 10\n",
    "\n",
    "        # Freeze the parameters of the base model.\n",
    "        # This is a crucial step in transfer learning. We don't want to update\n",
    "        # the weights of the pre-trained layers, only our new classifier.\n",
    "        ...\n",
    "\n",
    "        # Get the number of output features from the base model's last layer.\n",
    "        # For this EfficientNet model, this is found in the last pooling layer's output.\n",
    "        # This can vary between models, so you might need to inspect the model architecture.\n",
    "        in_features = ...\n",
    "\n",
    "        # Create a new linear layer for classification.\n",
    "        self.classifier = ...\n",
    "        print(\"Custom classifier built successfully.\")\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "        \"\"\"\n",
    "        # Pass the input through the base model.\n",
    "        outputs = ...\n",
    "\n",
    "        # The output from Hugging Face models often includes more than just the final\n",
    "        # layer's activations. We're interested in `last_hidden_state`.\n",
    "        # We then take the mean across the spatial dimensions to get a feature vector.\n",
    "        pooled_output = outputs.last_hidden_state.mean(dim=[2, 3])\n",
    "\n",
    "        # Pass the pooled output through our new classifier.\n",
    "        logits = ...\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97804cf0-ad58-418d-9ea5-5b9f8fecd106",
   "metadata": {},
   "source": [
    "### step 5. `train_model` \n",
    "This function contains the logic for training our model. It iterates over the training dataset\n",
    "for a specified number of `epochs`. In each iteration, it performs the standard PyTorch training\n",
    "steps: 1) get a batch of data, 2) perform a forward pass to get the model's predictions,\n",
    "3) calculate the loss (how wrong the predictions are) using the specified criterion,\n",
    "4) perform a backward pass (`loss.backward()`) to compute gradients, and 5) update the model's\n",
    "trainable weights (only our classifier layer) using the optimizer (`optimizer.step()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022f6f57-1c46-4124-b55a-1d779dcf6490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 5: Define the Training Loop ---\n",
    "def train_model(model, train_loader, criterion, optimizer, device, epochs=3):\n",
    "    \"\"\"\n",
    "    The main training loop.\n",
    "    \"\"\"\n",
    "    print(\"\\nStep 5: Starting the training process...\")\n",
    "    model.to(device) # Move model to the selected device (GPU/CPU)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train() # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        # Use tqdm for a nice progress bar\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        for images, labels in progress_bar:\n",
    "            # Move images and labels to the device\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # 1. Zero the parameter gradients\n",
    "            ...\n",
    "\n",
    "            # 2. Forward pass\n",
    "            outputs = ...\n",
    "            loss = ...\n",
    "\n",
    "            # 3. Backward pass and optimize\n",
    "            ...\n",
    "            ...\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"End of Epoch {epoch+1}, Average Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c962fcd-f459-4423-ba22-7fccda35e9a1",
   "metadata": {},
   "source": [
    "### step 6. `evaluate_model`\n",
    "\n",
    "After training, we need to evaluate how well our model performs on unseen data. This function\n",
    "iterates through the test dataset. For each batch, it gets the model's predictions and compares\n",
    "them to the true labels. It's important to set the model to `eval()` mode, which disables\n",
    "certain layers like dropout, and to use `torch.no_grad()` to stop PyTorch from calculating\n",
    "gradients, which makes the process faster and uses less memory. The function calculates and prints the final accuracy of the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6cbdfc-c512-4774-bc4d-2ecc81e9d693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 6: Define the Evaluation Loop ---\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model's performance on the test set.\n",
    "    \"\"\"\n",
    "    print(\"\\nStep 6: Evaluating the model...\")\n",
    "    model.to(device)\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Since we're not training, we don't need to calculate gradients\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Get model predictions\n",
    "            outputs = ...\n",
    "            \n",
    "            # Get the class with the highest score\n",
    "            _, predicted = ...\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Accuracy of the model on the {total} test images: {accuracy:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8399c717-1c3a-4296-9b3c-89fca3942c9a",
   "metadata": {},
   "source": [
    "## Putting everything together\n",
    "\n",
    "All code written above should \"fill\" the different function calls. Once all is done, the following cell should run and do the training! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0320dc1-8aad-4baa-b286-f5b189f6a934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 7: Main Execution ---\n",
    "# This block ties everything together.\n",
    "print(\"--- PyTorch Transfer Learning Exercise ---\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 1. Get Dataloaders\n",
    "train_loader, test_loader = get_cifar10_dataloaders()\n",
    "\n",
    "# 2. Load Base Model\n",
    "base_model = load_pretrained_model()\n",
    "\n",
    "# 3. Create Custom Model\n",
    "cifar_model = EfficientNetCIFAR10(base_model)\n",
    "\n",
    "# 4. Define Loss and Optimizer\n",
    "# We only want to train the parameters of our new classifier layer.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cifar_model.classifier.parameters(), lr=0.001)\n",
    "\n",
    "# 5. Train the Model\n",
    "# Note: Training for more epochs will yield better results.\n",
    "# 3 epochs is a good starting point to verify the setup works.\n",
    "train_model(cifar_model, train_loader, criterion, optimizer, device, epochs=3)\n",
    "\n",
    "# 6. Evaluate the Model\n",
    "evaluate_model(cifar_model, test_loader, device)\n",
    "\n",
    "print(\"\\nExercise Complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
