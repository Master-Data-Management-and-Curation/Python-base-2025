{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 7: Building Models in PyTorch with `nn.Module`\n",
    "\n",
    "**Objective:** To learn how to define, inspect, and modify neural network models in PyTorch by understanding layers, creating custom model classes, and leveraging pre-trained architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. PyTorch Layers: The Building Blocks\n",
    "\n",
    "The `torch.nn` namespace contains all the building blocks we need to create neural networks. Think of them as individual layers that perform specific operations. Crucially, these layers are themselves callable objects (like functions) that can process tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create a sample input tensor: 1 sample, 10 features\n",
    "input_tensor = torch.randn(1, 10)\n",
    "print(f\"Input Tensor Shape: {input_tensor.shape}\\n\")\n",
    "\n",
    "# --- Example 1: A Linear Layer ---\n",
    "# A linear layer applies a linear transformation: y = Wx + b\n",
    "# It takes in_features and maps them to out_features.\n",
    "linear_layer = nn.Linear(in_features=10, out_features=5)\n",
    "\n",
    "# Pass the input through the layer as if it were a function\n",
    "output_tensor = linear_layer(input_tensor)\n",
    "print(f\"--- Linear Layer ---\")\n",
    "print(f\"Output Tensor Shape: {output_tensor.shape}\")\n",
    "print(f\"Output Tensor Values:\\n {output_tensor}\\n\")\n",
    "\n",
    "# --- Example 2: An Activation Function ---\n",
    "# A ReLU (Rectified Linear Unit) activation is a non-linear operation.\n",
    "relu_layer = nn.ReLU()\n",
    "activated_tensor = relu_layer(output_tensor)\n",
    "print(f\"--- ReLU Layer ---\")\n",
    "print(f\"Activated Tensor Values (negative values become 0):\\n {activated_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `nn.Linear` is a *class*. \n",
    "When you write  `nn.Linear(in_features=10, out_features=5)` instead you are created a \"real\" linear layer which is an instance of that class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Building a Model with `nn.Module`\n",
    "\n",
    "To create a full model, we group layers together in a logical structure. The standard way to do this in PyTorch is to create a class that inherits from `nn.Module`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is inherited from `nn.Module`? Most importantly:\n",
    "1. **\"Parameter tracking\"**. When you add a layer to a `nn.Module`, it actually constructs a robust structure where everything is accounted for (see e.g. the `module.parameters()` call)\n",
    "2. **\"State management\"**. The methods to easily save and load the model etc. etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A custom model class has two essential parts:\n",
    "1.  `__init__(self)`: This is where you **define** all the layers your model will use. You instantiate them here and assign them as attributes of the class.\n",
    "2.  `forward(self, x)`: This is where you **define the data flow**. You take an input tensor `x` and pass it through the layers you defined in `__init__` in the correct sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        \n",
    "        # 1. Define the layers\n",
    "        # From their respective class blueprints -> generate the instances.\n",
    "        self.layer1 = nn.Linear(in_features=10, out_features=32)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(in_features=32, out_features=2) # Output 2 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 2. Define the forward pass\n",
    "        x = self.layer1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Inspecting a Model\n",
    "\n",
    "Once you have a model instance, you can easily inspect its structure and access its learnable parameters. The `.parameters()` method is particularly important, as this is what you pass to an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model architecture\n",
    "print(\"--- Model Architecture ---\")\n",
    "print(model)\n",
    "\n",
    "# Access and inspect learnable parameters\n",
    "print(\"\\n--- Model Parameters ---\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Layer: {name}, Shape: {param.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Using Pre-trained Models\n",
    "\n",
    "Training large models from scratch is computationally expensive. **Transfer Learning** is the practice of taking a powerful model that has already been trained on a massive dataset (like ImageNet) and adapting it for your own task. `torchvision.models` provides many such models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# Load a pre-trained ResNet-18 model\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "\n",
    "# Print the last few layers of the model to see its structure\n",
    "print(\"--- Original ResNet-18 Final Layer ---\")\n",
    "print(resnet18.fc) # .fc is the name of the final fully connected layer in ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resnet18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Modifying a Pre-trained Model (Finetuning)\n",
    "\n",
    "The most common transfer learning technique is to replace the final layer of a pre-trained model. The original model was trained to classify 1000 ImageNet classes. If our task is to classify 10 different classes (e.g., CIFAR-10), we need a final layer that outputs 10 values, not 1000.\n",
    "\n",
    "The process is:\n",
    "1.  **Freeze** the weights of the pre-trained layers so they don't change during training.\n",
    "2.  **Replace** the final classification layer with a new one tailored to your task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Freeze all the parameters in the network\n",
    "for param in resnet18.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 2. Replace the final layer\n",
    "num_features = resnet18.fc.in_features # Get the number of input features for the last layer\n",
    "num_classes = 10 # Our new task has 10 classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new final layer and assign it\n",
    "resnet18.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "print(\"--- Modified ResNet-18 Final Layer ---\")\n",
    "print(resnet18.fc)\n",
    "\n",
    "# Verify that only the new layer's parameters require gradients\n",
    "print(\"\\n--- Trainable Parameters After Modification ---\")\n",
    "for name, param in resnet18.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Exercises & Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 7.1: Build a Simple Multi-Layer Perceptron (MLP) for MNIST\n",
    "\n",
    "* **Task:** Create a simple MLP to classify handwritten digits from the MNIST dataset.\n",
    "  1. Define a class `SimpleMLP` that inherits from `nn.Module`.\n",
    "  2. In `__init__`, define the following layers:\n",
    "     - A flatten layer (`nn.Flatten`) to convert the 28x28 images into a 1D vector.\n",
    "     - A linear layer that takes the flattened image (784 features) and maps it to a hidden dimension of 128 features.\n",
    "     - A ReLU activation.\n",
    "     - A second linear layer that maps the 128 hidden features to the 10 output classes (digits 0-9).\n",
    "  3. Define the `forward` pass to connect these layers in sequence.\n",
    "  4. Instantiate your MLP and pass a dummy image tensor through it to verify the output shape. A dummy tensor for a single MNIST image would have the shape `(1, 1, 28, 28)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Your Code Here ---\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = ...\n",
    "        return x\n",
    "\n",
    "# Instantiate and test\n",
    "mlp = SimpleMLP()\n",
    "dummy_image = torch.randn(1, 1, 28, 28) # (batch, channels, height, width)\n",
    "output = mlp(dummy_image)\n",
    "\n",
    "print(\"--- SimpleMLP Architecture ---\")\n",
    "print(mlp)\n",
    "print(f\"\\nOutput shape for a dummy image: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Prepare the MNIST Dataset\n",
    "# Define a transform to normalize the data and convert images to tensors\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Download and load the training data\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 3. Instantiate the Model, Loss Function, and Optimizer\n",
    "model = SimpleMLP()\n",
    "criterion = nn.CrossEntropyLoss() # CrossEntropyLoss is ideal for multi-class classification\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 4. The Training Loop\n",
    "num_epochs = 5\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if (i + 1) % 100 == 0: # Print status every 100 batches\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# 5. Evaluate the Model (Optional but good practice)\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad(): # We don't need to calculate gradients during evaluation\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
