{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 5: PyTorch Fundamentals - Tensors and Gradients\n",
    "\n",
    "**Objective:** To introduce the core concepts of PyTorch, focusing on its fundamental data structure (the Tensor) and its automatic differentiation engine (`autograd`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Concepts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is PyTorch?\n",
    "\n",
    "PyTorch is an open-source machine learning library. It's widely used for applications such as computer vision and natural language processing. Its two core features are:\n",
    "\n",
    "- **Tensors:** A multi-dimensional array, similar to NumPy's `ndarray`, but with the ability to run on GPUs for accelerated computing.\n",
    "- **Automatic Differentiation:** A system called `torch.autograd` that automatically calculates gradients, which are essential for training neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tensors: The Building Blocks\n",
    "Everything in PyTorch is based on Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# --- Creating Tensors ---\n",
    "\n",
    "# From a Python list\n",
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "print(f\"Tensor from list:\\n {x_data}\\n\")\n",
    "\n",
    "# From a NumPy array (and vice-versa)\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "print(f\"Tensor from NumPy array:\\n {x_np}\\n\")\n",
    "\n",
    "np_from_tensor = x_np.numpy()\n",
    "print(f\"NumPy array from Tensor:\\n {np_from_tensor}\\n\")\n",
    "\n",
    "# Tensors of ones, zeros, and random numbers\n",
    "shape = (2, 3,)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "rand_tensor = torch.rand(shape)\n",
    "\n",
    "print(f\"Ones Tensor:\\n {ones_tensor} \\n\")\n",
    "print(f\"Random Tensor:\\n {rand_tensor} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tensor Operations\n",
    "Operations have a syntax similar to NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "\n",
    "# --- Indexing and Slicing ---\n",
    "print(f\"First row: {tensor[0]}\")\n",
    "print(f\"First column: {tensor[:, 0]}\")\n",
    "print(f\"Last column: {tensor[..., -1]}\\n\")\n",
    "\n",
    "# --- Arithmetic Operations ---\n",
    "y1 = tensor + tensor\n",
    "y2 = tensor * tensor\n",
    "\n",
    "# Matrix multiplication\n",
    "y3 = tensor.matmul(tensor.T) # Using .matmul()\n",
    "y4 = tensor @ tensor.T      # Using the @ operator (more common)\n",
    "\n",
    "print(f\"Matrix Multiplication Result:\\n {y4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Automatic Differentiation with `torch.autograd`\n",
    "This is the magic of PyTorch. When training a model, you need to calculate the gradient of the loss function with respect to the model's parameters. PyTorch does this automatically.\n",
    "\n",
    "- We can tell PyTorch to track operations on a tensor by setting `requires_grad=True`.\n",
    "- When we finish our computation (e.g., `y = 3*x**2`), we can call `y.backward()`.\n",
    "- PyTorch will then compute the gradients `dy/dx` and store them in the `.grad` attribute of the tensor `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor and set requires_grad=True to track computation\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Define a simple function\n",
    "y = 3*x**2 + 5\n",
    "# In math, the derivative dy/dx is 6*x. At x=2, the gradient should be 12.\n",
    "\n",
    "# Use autograd to calculate the gradients\n",
    "y.backward()\n",
    "\n",
    "# The calculated gradient is now stored in x.grad\n",
    "print(f\"The gradient dy/dx at x=2 is: {x.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Disabling Gradient Tracking with `torch.no_grad()`\n",
    "\n",
    "By default, tensors with `requires_grad=True` will track their history. However, there are times when we don't need this, especially during model evaluation (inference). Disabling it saves memory and speeds up computation.\n",
    "\n",
    "We do this using a `with torch.no_grad():` block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.tensor(5.0, requires_grad=True)\n",
    "print(f\"z requires grad: {z.requires_grad}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Inside this block, operations are not tracked\n",
    "    q = z**2\n",
    "\n",
    "print(f\"q was created without tracking, so requires_grad is: {q.requires_grad}\")\n",
    "\n",
    "# This would cause an error because the computation history for q was not saved:\n",
    "try:\n",
    "    q.backward()\n",
    "except RuntimeError as e:\n",
    "    print(f\"\\nError trying to call backward() on q: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Exercises & Debugging (90 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 5.1: Tensor Creation and Manipulation\n",
    "* **Task:** \n",
    "  1. Create a 3x3 tensor with random values.\n",
    "  2. Create a 3x3 tensor of all ones.\n",
    "  3. Add the two tensors together.\n",
    "  4. Multiply the resulting tensor by a scalar (e.g., 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "rand_t = torch.rand(3, 3)\n",
    "ones_t = torch.ones(3, 3)\n",
    "\n",
    "added_t = rand_t + ones_t\n",
    "final_t = added_t * 5\n",
    "\n",
    "print(\"Final result of Lab 5.1:\")\n",
    "print(final_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 5.2: Simple Linear Equation\n",
    "* **Task:** Manually perform the calculation for a simple linear layer: `y = Wx + b`.\n",
    "  1. Create a weight tensor `W` of shape (1, 3) with random values.\n",
    "  2. Create an input tensor `x` of shape (3, 1) with random values.\n",
    "  3. Create a bias tensor `b` of shape (1, 1) with a value of 0.5.\n",
    "  4. Calculate `y` using matrix multiplication (`@`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "W = torch.rand(1, 3)\n",
    "x_in = torch.rand(3, 1)\n",
    "b = torch.tensor([[0.5]])\n",
    "\n",
    "y_out = W @ x_in + b\n",
    "\n",
    "print(\"Result of y = Wx + b:\")\n",
    "print(y_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 5.3: Practice with Autograd\n",
    "* **Task:** Calculate the gradient for the function `z = 2a^3 + 3b`.\n",
    "  1. Create tensors `a` and `b` with values `2.0` and `5.0` respectively. Make sure to set `requires_grad=True`.\n",
    "  2. Calculate `z`.\n",
    "  3. Call `backward()` on `z`.\n",
    "  4. Print the gradients stored in `a.grad` and `b.grad`.\n",
    "  5. **Question:** Before running, what do you expect the gradients `dz/da` and `dz/db` to be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Math check: dz/da = 6a^2. At a=2, this is 6 * 4 = 24.\n",
    "# Math check: dz/db = 3.\n",
    "\n",
    "a = torch.tensor(2.0, requires_grad=True)\n",
    "b = torch.tensor(5.0, requires_grad=True)\n",
    "\n",
    "z_calc = 2*a**3 + 3*b\n",
    "\n",
    "z_calc.backward()\n",
    "\n",
    "print(f\"The gradient dz/da is: {a.grad}\")\n",
    "print(f\"The gradient dz/db is: {b.grad}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
